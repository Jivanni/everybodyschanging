{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd69750b-39e9-4433-bb87-328980f4c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7db39a7-5251-41a3-90f9-0a3178aa6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33447177-1528-4e97-9fff-3e43d0755f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ab2610d-b243-4be2-b508-e80f0a7cd9ed",
   "metadata": {},
   "source": [
    "## Loading files and removing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c704474e-2946-452f-9ad3-42f6baf27298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"songlyrics/songs_with_filenames_and_feats_6.csv\", delimiter=\";\")\n",
    "df = df[df['filename'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1c9ad9-307c-430c-9eba-fd5554458b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"songlyrics/lyrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823c202c-59c6-414c-a5df-0a1adc8a2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = os.listdir(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080dc7d2-40fd-4b59-aa47-f987c5b61a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6671 unique files\n"
     ]
    }
   ],
   "source": [
    "print(f\"found {len(songs)} unique files\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de739f-30b6-46d9-82f1-08ee26a43c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2fabd85-ede8-4da8-b4cf-4b88d8a93822",
   "metadata": {},
   "source": [
    "## String Processing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4645980-b624-4472-a5e7-d3d27447cbf4",
   "metadata": {},
   "source": [
    "We extract tokens, bigrams and trigrams from each lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0174f8c2-2ae3-4f9c-a9b5-08ad3108cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grams(text):\n",
    "    toks = word_tokenize(text)\n",
    "    bigs = [a + \"_\" + b for a,b in nltk.bigrams(toks)]\n",
    "    trigs = [a + \"_\" + b + \"_\" + c for a,b,c in nltk.trigrams(toks)]\n",
    "    return toks + bigs + trigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76115755-2dc2-45e6-aa5c-70d9a72b547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for song in df[\"filename\"]:\n",
    "    with open(os.path.join(DATA,song),\"r\",encoding='utf-8') as file:\n",
    "        lyrics = get_grams(file.read().lower())\n",
    "        data.append(TaggedDocument(lyrics ,(song,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233f65c-ea82-4bf2-a3e6-0ad9f67d5f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f581a30-82a0-4d79-9060-63ea4a86b719",
   "metadata": {},
   "source": [
    "## Word2Vec model training on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558288b-71e9-466b-816e-6de5773c8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "model = Doc2Vec(data, vector_size=100, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90d3b8-38e8-472c-8cc4-b92e772928b8",
   "metadata": {},
   "source": [
    "## We save the Doc2Vec vectors to file for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39cb355-5686-47d0-a3ff-dfcf9bcf062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_vectors = model.dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fab19d-5e62-4e36-9cb1-ef2453d1cbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-23 16:12:23,214 : INFO : KeyedVectors lifecycle event {'fname_or_handle': 'word2vecmodels/docs_grams_final.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-06-23T16:12:23.214456', 'gensim': '4.0.1', 'python': '3.8.10 (default, Jun  4 2021, 15:09:15) \\n[GCC 7.5.0]', 'platform': 'Linux-5.11.0-7614-generic-x86_64-with-glibc2.17', 'event': 'saving'}\n",
      "2021-06-23 16:12:23,221 : INFO : saved word2vecmodels/docs_grams_final.model\n"
     ]
    }
   ],
   "source": [
    "doc2vec_vectors.save(\"word2vecmodels/docs_grams_final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea13ec-7f68-4d69-8de3-ed06850551f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a57c3-816a-4d77-9a41-7585d784b837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
